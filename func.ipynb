{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure OpenAI Assistants API with functions\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# Load environment variables from .env file\n",
    "# AZURE_OPENAI_API_KEY\n",
    "# AZURE_OPENAI_ENDPOINT\n",
    "# AZURE_OPENAI_API_VERSION\n",
    "# SEARCH_KEY\n",
    "load_dotenv()\n",
    "\n",
    "# Create Azure OpenAI client\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv('AZURE_OPENAI_API_KEY'),\n",
    "    azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'),\n",
    "    api_version=os.getenv('AZURE_OPENAI_API_VERSION')\n",
    ")\n",
    "\n",
    "# assistant ID as created in the portal\n",
    "assistant_id = \"asst_Z2YGBjhORYJGyPv6AQ4HugzP\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a thread\n",
    "\n",
    "A thread is not linked to the assistant at creation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread id:  thread_zPVdxn0yxRysszn5g9kkCSVo\n"
     ]
    }
   ],
   "source": [
    "# Create a thread\n",
    "thread = client.beta.threads.create()\n",
    "\n",
    "# Threads have an id as well\n",
    "print(\"Thread id: \", thread.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a message to the thread\n",
    "\n",
    "The assistant can answer questions by searching the blog, blog.baeke.info. It should do that with the search_blog function as defined in the assistant.\n",
    "\n",
    "The function json in the assistant is:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"name\": \"search_blog\",\n",
    "  \"description\": \"Search blog.baeke.info\",\n",
    "  \"parameters\": {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "      \"query\": {\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"Query used by the search\"\n",
    "      }\n",
    "    },\n",
    "    \"required\": [\n",
    "      \"query\"\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "The function takes one parameter: the query.\n",
    "\n",
    "After adding the user message and running the thread, we show information about the run via a json dump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"run_afYakj2F9k8bXta0qZUE0oLI\",\n",
      "  \"assistant_id\": \"asst_Z2YGBjhORYJGyPv6AQ4HugzP\",\n",
      "  \"cancelled_at\": null,\n",
      "  \"completed_at\": null,\n",
      "  \"created_at\": 1707430061,\n",
      "  \"expires_at\": 1707430661,\n",
      "  \"failed_at\": null,\n",
      "  \"file_ids\": [],\n",
      "  \"instructions\": \"You answer questions about the blog of Geert Baeke. The blog can be found at https://blog.baeke.info.\\n\\nEvery technical question you get should be answered by a function. If the results of the function are not relevant, inform the user.\\n\",\n",
      "  \"last_error\": null,\n",
      "  \"metadata\": {},\n",
      "  \"model\": \"gpt-4-preview\",\n",
      "  \"object\": \"thread.run\",\n",
      "  \"required_action\": {\n",
      "    \"submit_tool_outputs\": {\n",
      "      \"tool_calls\": [\n",
      "        {\n",
      "          \"id\": \"call_HDEGQgUUTA1LFEcRxbj7DJTh\",\n",
      "          \"function\": {\n",
      "            \"arguments\": \"{\\\"query\\\":\\\"OpenAI Assistant API\\\"}\",\n",
      "            \"name\": \"search_blog\"\n",
      "          },\n",
      "          \"type\": \"function\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    \"type\": \"submit_tool_outputs\"\n",
      "  },\n",
      "  \"started_at\": 1707430062,\n",
      "  \"status\": \"requires_action\",\n",
      "  \"thread_id\": \"thread_zPVdxn0yxRysszn5g9kkCSVo\",\n",
      "  \"tools\": [\n",
      "    {\n",
      "      \"function\": {\n",
      "        \"name\": \"search_blog\",\n",
      "        \"description\": \"Search blog.baeke.info\",\n",
      "        \"parameters\": {\n",
      "          \"type\": \"object\",\n",
      "          \"properties\": {\n",
      "            \"query\": {\n",
      "              \"type\": \"string\",\n",
      "              \"description\": \"Query used by the search\"\n",
      "            }\n",
      "          },\n",
      "          \"required\": [\n",
      "            \"query\"\n",
      "          ]\n",
      "        }\n",
      "      },\n",
      "      \"type\": \"function\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# function returns the run when status is no longer queued or in_progress\n",
    "def wait_for_run(run, thread_id):\n",
    "    while run.status == 'queued' or run.status == 'in_progress':\n",
    "        run = client.beta.threads.runs.retrieve(\n",
    "                thread_id=thread_id,\n",
    "                run_id=run.id\n",
    "        )\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    return run\n",
    "\n",
    "\n",
    "# create a message\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"What is the meaning of life?\"\n",
    ")\n",
    "\n",
    "# create a run \n",
    "run = client.beta.threads.runs.create(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant_id # use the assistant id defined in the first cell\n",
    ")\n",
    "\n",
    "# wait for the run to complete\n",
    "run = wait_for_run(run, thread.id)\n",
    "\n",
    "# show information about the run\n",
    "# should indicate that run status is requires_action\n",
    "# should contain information about the tools to call\n",
    "print(run.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function to search the blog\n",
    "\n",
    "The search_blog function is a helper function that uses the requests library to search the blog. It returns multiple results via a similarity searh in Azure AI Search.\n",
    "\n",
    "We could query Azure AI Search directly here but the API that is used was already created and running as an Azure Container App."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search the blog\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def search_blog(api_key, query):\n",
    "    url = \"https://myblog.gentlebay-4474176e.westeurope.azurecontainerapps.io/generate_response\"\n",
    "    headers = {\"api-key\": api_key}\n",
    "    data = {\"query\": query}\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return response.status_code, response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking if we need to use a tool\n",
    "\n",
    "Below we check if we need to use a tool. We assume we need to here. We are not taking into account a scenario where we do not need to use a tool. In reality, we would need to allow for that scenario.\n",
    "\n",
    "If the assistant indicates we need to use a tool, it will tell use the function name and the arguments to use based on the function definition defined in the assistant. We will then call the search_blog function with the arguments and pass the tool call results back to the assistant.\n",
    "\n",
    "After passing the tool call results, we run the thread again and show the messages via a json dump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool calls: [RequiredActionFunctionToolCall(id='call_HDEGQgUUTA1LFEcRxbj7DJTh', function=Function(arguments='{\"query\":\"OpenAI Assistant API\"}', name='search_blog'), type='function')]\n",
      "Search result: {'response': [{'title': 'Trying the OpenAI Assistants API', 'content': 'If you have ever tried to build an AI assistant, you know that is not a simple task. In almost all cases, your assistant needs access to external knowledge such as documents or APIs. You might even want to provide your assistant a code sandbox to solve user queries with code. When your assistant is accessed via a chat application, you also have to implement chat history.\\nAlthough there are several frameworks like LangChain and Semantic Kernel that can help, OpenAI recently released the Assistants API. It is their own API, tied to their models. The primitives of an assistant are Assistants, Threads and Runs. Letâ€™s start by creating an assistant.\\nNote: this post contains code snippets in Python. You can find the full example in this gist: https://gist.github.com/gbaeke/e6e88c0dc68af3aa4a89b1228012ae53\\nNote: although I except this API to become available in Azure OpenAI, I am not quite sure it will happen fast, if at all. So for now, try it out at OpenAI directly. It is still in beta!\\nCreating an assistant\\nYou can create an assistant using the portal or from code. An assistant has several parameters:\\n\\nInstructions: how should the assistant behave or respond; think of it as the system message\\nModel: use any supported model, including fine-tuned models; to support retrieval from documents, you need the 1106 version of gpt-3.5-turbo/gpt-4\\nTools: currently, the API supports Code Interpreter and Retrieval; these are fully hosted by OpenAI\\nFunctions: define custom functions to call to integrate with external APIs for instance', 'url': 'https://blog.baeke.info/2023/11/14/trying-the-openai-assistants-api/'}, {'title': 'Step-by-Step Guide: How to Build Your Own Chatbot with the ChatGPT API', 'content': 'Although, at the start, the responses follow the system message, the assistant starts to correct itself and answers correctly. As stated, user messages eventually carry more weight.\\nSummary\\nIn this post, we discussed how to build a chat bot using the ChatGPT API and Python. We went through the setup process, created an OpenAI account, and wrote the chat bot code using the OpenAI API. The bot used the ChatCompletion API and maintained context in the conversation by storing and sending previous messages to the API at each request. We also discussed counting tokens and truncating the message list to avoid exceeding the maximum token limit for the model. The full code is available on GitHub, and we provided an example conversation between the bot and the user. The post aimed to guide both beginning developers and beginners in AI and chat bot development through the step-by-step process of building their chat bot using the ChatGPT API and keep it as simple as possible. \\nHope you liked it!\\nShare this:TwitterFacebookLinkedInEmailRedditPinterestPocketWhatsAppLike this:Like Loading...', 'url': 'https://blog.baeke.info/2023/03/12/step-by-step-guide-how-to-build-your-own-chatbot-with-the-chatgpt-api/'}, {'title': 'Trying the OpenAI Assistants API', 'content': 'The above should be pretty clear:\\n\\nif the assistant responds with text, display the text\\nif the assistant responds with an image, there is an image Id; I use a get_content function to download the image from OpenIA; get_content also implements some straightforward caching logic to avoid having to download images over and over again in the same thread\\n\\nThe get_content function uses client.files.content(file_id).response.content to retrieve the file (client is OpenAI client). The returned result can be used by PIL to open the image and subsequently display it with Streamlitâ€™s st.image:\\nAssistant in a Streamlit app\\nNote that I can keep asking questions, which adds messages to the same thread, based on the threadâ€™s Id in Streamlitâ€™s session state. When the user refreshes the browser, session state is cleared so a new thread is started. For example, when I ask change 2x in 3x:\\nAsking to change the function\\nIn the code, I do not have to worry about chat history at all. I just add messages to the thread, which is managed by OpenAI. At the next run, all those messages are sent to the assistantâ€™s model, which responds appropriately. Note that you do pay for the tokens that all those messages consume.\\nConclusion\\nCompared to the synchronous and stateless ChatCompletion API, the Assistants API is asynchronous and stateful. As a developer, you create an assistant with tools, functions and content for retrieval purposes. Interacting with the assistant is easy: simply add messages to a thread and create a run. \\nObviously, it is early days for this API as it is still in beta. Personally, I think itâ€™s a great step forward, making it easier to create quite sophisticated assistants. Most orchestration frameworks and AI tools like LangChain, Semantic Kernel, Flowise, etcâ€¦ already have support or will support assistants and will add extra capabilities or ease of use on top of the base functionality.\\nShare this:TwitterFacebookLinkedInEmailRedditPinterestPocketWhatsAppLike this:Like Loading...', 'url': 'https://blog.baeke.info/2023/11/14/trying-the-openai-assistants-api/'}, {'title': 'Trying the OpenAI Assistants API', 'content': 'Instructions: how should the assistant behave or respond; think of it as the system message\\nModel: use any supported model, including fine-tuned models; to support retrieval from documents, you need the 1106 version of gpt-3.5-turbo/gpt-4\\nTools: currently, the API supports Code Interpreter and Retrieval; these are fully hosted by OpenAI\\nFunctions: define custom functions to call to integrate with external APIs for instance\\n\\nNote that the retrieval tool supports uploaded files. There is no need for your own search solution (e.g., vector database with support for vector search, hybrid search, etcâ€¦). This is great in simpler scenarios where a full-fledged search system is not required. More control over retrieval will come later.\\nIn this post, we will focus on an assistant that uses Code Interpreter. You can simply create the assistant in the portal. You can see the instructions, model, tools and files:\\nAssistant with only the Code interpreter tool using the latest gpt-4 model\\nTo create this assistant, make sure you have an account at https://platform.openai.com. Create the assistant from the Assistants section:\\nCreating an assistant\\nAssistants have an id. For example, my assistant has this id: asst_VljToh6vQ1Mbu6Ct5L6qgpfy. I can use this id in my code to start creating threads.\\nBefore talking about threads, letâ€™s look at creating the assistant with code:\\n\\nassistant = client.beta.assistants.create(\\n                name=\"Math Tutor\",\\n                instructions=\"You are a personal math tutor. Write and run code to answer math questions.\",\\n                tools=[{\"type\": \"code_interpreter\"}],\\n                model=\"gpt-4-1106-preview\"\\n  )', 'url': 'https://blog.baeke.info/2023/11/14/trying-the-openai-assistants-api/'}, {'title': 'Building a chatbot in Azure that works with your data', 'content': 'The solution requires Azure Cognitive Search which is an extra cost. The minimum cost is around 70 euros per month. There are open-source solutions you can use for free or SaaS solutions that provide a free option (e.g., Pinecone). Azure OpenAI on your data only supports Azure Cognitive Search for now although technically, Microsoft could open this up to other stores.\\nAzure Cognitive Search is somewhat more complex than (some) vector databases such as Pinecone or Chroma. If you want to use other search engines/vector databases, I recommend using LangChain in combination with something like Chainlit to create your prototype. Of course, that means you will have to write more code. No more wizards for you! ðŸ˜ƒ\\nThe source code for the web app is at https://github.com/microsoft/sample-app-aoai-chatGPT. Although the code is not super complex, Python tools such as Streamlit and Chainlit make it much easier to create a prototype from scratch. Note that the web app is protected with Azure Active Directory by default and that it authenticates to Cognitive Search and Azure OpenAI using API keys set as environment variables. This is all automatically configured for you!\\nAzure Cognitive Search integration is part of the Azure OpenAI API version 2023-06-01 and depends on a dataSources field in the JSON body sent to the Azure OpenAI API. Check the source code here. I would have preferred the API to stay aligned with the OpenAI APIs and retrieve extra content as a separate step.\\n\\nWith all this being said, if all you need for your demo is the web app generated by the Chat playgroundâ€™s Deploy button, this is one of the quickest ways to get there!\\nTo see the entire experience in action, check out the video below or click this link: https://www.youtube.com/watch?v=gySeOggsz-w.\\n\\n\\n\\nShare this:TwitterFacebookLinkedInEmailRedditPinterestPocketWhatsAppLike this:Like Loading...', 'url': 'https://blog.baeke.info/2023/07/29/building-a-chatbot-based-on-your-documents-in-azure/'}]}\n",
      "Tool outputs submitted\n",
      "Run information:\n",
      "----------------\n",
      "{\n",
      "  \"id\": \"run_afYakj2F9k8bXta0qZUE0oLI\",\n",
      "  \"assistant_id\": \"asst_Z2YGBjhORYJGyPv6AQ4HugzP\",\n",
      "  \"cancelled_at\": null,\n",
      "  \"completed_at\": 1707430099,\n",
      "  \"created_at\": 1707430061,\n",
      "  \"expires_at\": null,\n",
      "  \"failed_at\": null,\n",
      "  \"file_ids\": [],\n",
      "  \"instructions\": \"You answer questions about the blog of Geert Baeke. The blog can be found at https://blog.baeke.info.\\n\\nEvery technical question you get should be answered by a function. If the results of the function are not relevant, inform the user.\\n\",\n",
      "  \"last_error\": null,\n",
      "  \"metadata\": {},\n",
      "  \"model\": \"gpt-4-preview\",\n",
      "  \"object\": \"thread.run\",\n",
      "  \"required_action\": null,\n",
      "  \"started_at\": 1707430087,\n",
      "  \"status\": \"completed\",\n",
      "  \"thread_id\": \"thread_zPVdxn0yxRysszn5g9kkCSVo\",\n",
      "  \"tools\": [\n",
      "    {\n",
      "      \"function\": {\n",
      "        \"name\": \"search_blog\",\n",
      "        \"description\": \"Search blog.baeke.info\",\n",
      "        \"parameters\": {\n",
      "          \"type\": \"object\",\n",
      "          \"properties\": {\n",
      "            \"query\": {\n",
      "              \"type\": \"string\",\n",
      "              \"description\": \"Query used by the search\"\n",
      "            }\n",
      "          },\n",
      "          \"required\": [\n",
      "            \"query\"\n",
      "          ]\n",
      "        }\n",
      "      },\n",
      "      \"type\": \"function\"\n",
      "    }\n",
      "  ]\n",
      "} \n",
      "\n",
      "Messages in the thread:\n",
      "-----------------------\n",
      "{\n",
      "  \"data\": [\n",
      "    {\n",
      "      \"id\": \"msg_WjD3phZfGg3ORufpNDDLVCeg\",\n",
      "      \"assistant_id\": \"asst_Z2YGBjhORYJGyPv6AQ4HugzP\",\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"text\": {\n",
      "            \"annotations\": [],\n",
      "            \"value\": \"The OpenAI Assistant API is a recent API released by OpenAI, which provides a framework for building AI assistants. It allows users to create an assistant with various parameters, such as instructions for behavior, model specifications (supporting both GPT-3.5 and GPT-4), and tools like Code Interpreter and Retrieval which are fully hosted by OpenAI.\\n\\nThe API introduces the primitives of Assistants, Threads, and Runs. Assistants can be created either via a portal or from code, each having its own ID. When creating an assistant, different tools and functions can be defined, such as custom functions to integrate with external APIs. Developers can easily interact with the assistant by adding messages to a thread and then creating a run. The Assistants API is designed to be asynchronous and stateful, in contrast to the synchronous and stateless ChatCompletion API.\\n\\nGeert Baeke has written a blog post detailing his experience with trying out the Assistants API, including code snippets and a comprehensive explanation of the features. For more in-depth information, you might want to read his blog post, titled [\\\"Trying the OpenAI Assistants API,\\\"](https://blog.baeke.info/2023/11/14/trying-the-openai-assistants-api/) where he also provides a link to the full example on GitHub.\"\n",
      "          },\n",
      "          \"type\": \"text\"\n",
      "        }\n",
      "      ],\n",
      "      \"created_at\": 1707430088,\n",
      "      \"file_ids\": [],\n",
      "      \"metadata\": {},\n",
      "      \"object\": \"thread.message\",\n",
      "      \"role\": \"assistant\",\n",
      "      \"run_id\": \"run_afYakj2F9k8bXta0qZUE0oLI\",\n",
      "      \"thread_id\": \"thread_zPVdxn0yxRysszn5g9kkCSVo\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"msg_ZMYwMqKfTdbGArGzVIA1DcH0\",\n",
      "      \"assistant_id\": null,\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"text\": {\n",
      "            \"annotations\": [],\n",
      "            \"value\": \"What is the OpenAI Assistant API?\"\n",
      "          },\n",
      "          \"type\": \"text\"\n",
      "        }\n",
      "      ],\n",
      "      \"created_at\": 1707430061,\n",
      "      \"file_ids\": [],\n",
      "      \"metadata\": {},\n",
      "      \"object\": \"thread.message\",\n",
      "      \"role\": \"user\",\n",
      "      \"run_id\": null,\n",
      "      \"thread_id\": \"thread_zPVdxn0yxRysszn5g9kkCSVo\"\n",
      "    }\n",
      "  ],\n",
      "  \"object\": \"list\",\n",
      "  \"first_id\": \"msg_WjD3phZfGg3ORufpNDDLVCeg\",\n",
      "  \"last_id\": \"msg_ZMYwMqKfTdbGArGzVIA1DcH0\",\n",
      "  \"has_more\": false\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# we only check for required_action here\n",
    "# required action means we need to call a tool\n",
    "if run.required_action:\n",
    "    # get tool calls and print them\n",
    "    # check the output to see what tools_calls contains\n",
    "    tool_calls = run.required_action.submit_tool_outputs.tool_calls\n",
    "    print(\"Tool calls:\", tool_calls)\n",
    "\n",
    "    # we might need to call multiple tools\n",
    "    # the assistant API supports parallel tool calls\n",
    "    # we account for this here although we only have one tool call\n",
    "    tool_outputs = []\n",
    "    for tool_call in tool_calls:\n",
    "        func_name = tool_call.function.name\n",
    "        arguments = json.loads(tool_call.function.arguments)\n",
    "\n",
    "        # call the function with the arguments provided by the assistant\n",
    "        if func_name == \"search_blog\":\n",
    "            result = search_blog(os.getenv('SEARCH_KEY'), **arguments)\n",
    "            print(\"Search result:\", result)\n",
    "\n",
    "        # append the results to the tool_outputs list\n",
    "        # you need to specify the tool_call_id so the assistant knows which tool call the output belongs to\n",
    "        tool_outputs.append({\n",
    "            \"tool_call_id\": tool_call.id,\n",
    "            \"output\": json.dumps(result)\n",
    "        })\n",
    "\n",
    "    # now that we have the tool call outputs, pass them to the assistant\n",
    "    run = client.beta.threads.runs.submit_tool_outputs(\n",
    "        thread_id=thread.id,\n",
    "        run_id=run.id,\n",
    "        tool_outputs=tool_outputs\n",
    "    )\n",
    "\n",
    "    print(\"Tool outputs submitted\")\n",
    "\n",
    "    # now we wait for the run again\n",
    "    run = wait_for_run(run, thread.id)\n",
    "else:\n",
    "    print(\"No tool calls identified\\n\")\n",
    "\n",
    "# show information about the run\n",
    "print(\"Run information:\")\n",
    "print(\"----------------\")\n",
    "print(run.model_dump_json(indent=2), \"\\n\")\n",
    "\n",
    "# now print all messages in the thread\n",
    "print(\"Messages in the thread:\")\n",
    "print(\"-----------------------\")\n",
    "messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
    "print(messages.model_dump_json(indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
